{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cohere weave --q --disable-pip-version-check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave # < --- notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tool_calls(tool_calls):\n",
    "    # Determine the message based on the number of tool calls\n",
    "    if len(tool_calls) > 1:\n",
    "        print(\"The model suggests making Parallel Queries:\")\n",
    "    else:\n",
    "        print(\"The model suggests making a single tool call:\")\n",
    "\n",
    "    for i, tool_call in enumerate(tool_calls):\n",
    "        # If there's more than one tool call, separate each with a header\n",
    "        if len(tool_calls) > 1:\n",
    "            print(f\"== Parallel Tool Call #{i+1}\")\n",
    "\n",
    "        # Print the tool call name and \"with this code:\" on the same line\n",
    "        if tool_call.name == 'python_interpreter':\n",
    "            print(f\"{tool_call.name} with this code:\")\n",
    "            code = tool_call.parameters.get('code', '')\n",
    "            print(\"\\n\".join(f\"  {line}\" for line_num, line in enumerate(code.splitlines())))\n",
    "        else:\n",
    "            # For non-python_interpreter tool calls, just print the parameters\n",
    "            print(f\"{tool_call.name}\")\n",
    "            print(f\"{tool_call.parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Tools for Agentic RAG: a web search engine and a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. A web search engine, accessible through an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tavily-python --q --disable-pip-version-check\n",
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(api_key=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a web search engine\n",
    "@weave.op() # < ----- notice\n",
    "def web_search(query: str) -> list[dict]:\n",
    "  response = tavily_client.search(query, max_results=3)['results']\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the LLM is equipped with a description of the web search engine\n",
    "web_search_tool = {\n",
    "  \"name\": \"web_search\",\n",
    "  \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n",
    "  \"parameter_definitions\": {\n",
    "    \"query\": {\n",
    "      \"description\": \"Query to search the internet with\",\n",
    "      \"type\": \"str\",\n",
    "      \"required\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. A spreadsheet, accessible through a Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a python console, which can be used to access the spreadsheet, but also more generally to code and plot stuff\n",
    "import io, contextlib\n",
    "\n",
    "\n",
    "@weave.op() # < --- notice\n",
    "def python_interpreter(code: str) -> list[dict]:\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        # Redirect stdout to capture print statements\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            exec(code, globals())\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"executed_code\": code,\n",
    "            \"error\": str(e)\n",
    "\n",
    "        }]\n",
    "    # Get the output value\n",
    "    return [{\n",
    "  \t\t\"console_output\": output.getvalue(),\n",
    "      \"executed_code\": code\n",
    "  \t}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the LLM is equipped with a description of a python console\n",
    "python_interpreter_tool = {\n",
    "  \"name\": \"python_interpreter\",\n",
    "  \"description\": \"Executes python code and returns the result. The code runs in a static sandbox without internet access and without interactive mode, so print output or save output to a file.\",\n",
    "  \"parameter_definitions\": {\n",
    "    \"code\": {\n",
    "      \"description\": \"Python code to execute\",\n",
    "      \"type\": \"str\",\n",
    "      \"required\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"web_search\": web_search,\n",
    "    \"python_interpreter\": python_interpreter,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try Agentic RAG with a Cohere model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# < --- notice\n",
    "import cohere\n",
    "from weave.integrations.cohere import cohere_patcher\n",
    "\n",
    "cohere_patcher.attempt_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\almud\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dotenv usage:\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create cohere model and intergrate Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "# Integrate cohere with weave\n",
    "from weave.integrations.cohere import cohere_patcher\n",
    "\n",
    "cohere_patcher.attempt_patch()\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client using an environment variable for the API key\n",
    "cohere_api_key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "# Check if the .env file is loaded\n",
    "#print(f\"Cohere API Key: {api_key}\") \n",
    "\n",
    "\n",
    "co = cohere.Client(api_key=cohere_api_key)\n",
    "\n",
    "\n",
    "model = \"command-r-plus-08-2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Let's look at a complex user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble=\"\"\"You have access to to the internet.\n",
    "You also have access to a dataset with information about Spotify songs from the past 10 years, located at ./spotify_dataset.csv.\n",
    "Remember to inspect the dataset to understand its structure before querying it.\n",
    "Use the dataset when you can. Otherwise use the internet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What's the age and citizenship of the artists who had the top 3 most streamed songs on Spotify in 2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Get the model plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "#weave_client = \n",
    "weave_client = weave.init(\"cohere-weave-project\") # < --- notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f0-068b-7b22-9327-5ecac7a9f0d5\n",
      "I will first check the dataset to see if it contains information about the top 3 most streamed songs on Spotify in 2023. If it does, I will then find the age and citizenship of the artists of those songs.\n"
     ]
    }
   ],
   "source": [
    "response = co.chat(\n",
    "    model=model,\n",
    "    preamble=preamble,\n",
    "    message=message,\n",
    "    tools=[web_search_tool, python_interpreter_tool],\n",
    "    temperature=0,\n",
    "    prompt_truncation=\"OFF\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. RAG System Itarations for final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1;34m == STEP 1 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mI will first check the dataset to see if it contains information about the top 3 most streamed songs on Spotify in 2023. If it does, I will then find the age and citizenship of the artists of those songs.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "python_interpreter with this code:\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"spotify_dataset.csv\")\n",
      "  \n",
      "  # Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\n",
      "  if \"2023\" in df[\"year\"]: \n",
      "      print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "  else:\n",
      "      print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f0-6ac3-7ad2-bbbd-f4ff3541f190\n",
      "\u001b[1;35mtool_results:\u001b[0m \u001b[1m[{'executed_code': 'import pandas as pd\\n\\ndf = pd.read_csv(\"spotify_dataset.csv\")\\n\\n# Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\\nif \"2023\" in df[\"year\"]: \\n    print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\\nelse:\\n    print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")', 'error': \"'year'\"}]\u001b[0m\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f0-6d87-7fd2-a7d5-693ea1084094\n",
      "\n",
      "\n",
      "\u001b[1;34m == STEP 2 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mThe dataset does not contain information about the top 3 most streamed songs on Spotify in 2023. I will now search the web for this information.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "web_search\n",
      "{'query': 'top 3 most streamed songs on Spotify in 2023'}\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f0-8fae-7f90-be2a-5eadb7a93519\n"
     ]
    },
    {
     "ename": "InvalidAPIKeyError",
     "evalue": "The provided API key is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m tool_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m functions_map[tool_call\u001b[38;5;241m.\u001b[39mname](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_call\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m     18\u001b[0m     tool_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_call, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs}\n\u001b[0;32m     19\u001b[0m     tool_results\u001b[38;5;241m.\u001b[39mappend(tool_result)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:361\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:237\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:235\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_async()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     handle_exception(e)\n",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m, in \u001b[0;36mweb_search\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop() \u001b[38;5;66;03m# < ----- notice\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweb_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mtavily_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:98\u001b[0m, in \u001b[0;36mTavilyClient.search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     81\u001b[0m             query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     82\u001b[0m             search_depth: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvanced\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     93\u001b[0m             ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Combined search method.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search(query,\n\u001b[0;32m     99\u001b[0m                         search_depth\u001b[38;5;241m=\u001b[39msearch_depth,\n\u001b[0;32m    100\u001b[0m                         topic\u001b[38;5;241m=\u001b[39mtopic,\n\u001b[0;32m    101\u001b[0m                         days\u001b[38;5;241m=\u001b[39mdays,\n\u001b[0;32m    102\u001b[0m                         max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[0;32m    103\u001b[0m                         include_domains\u001b[38;5;241m=\u001b[39minclude_domains,\n\u001b[0;32m    104\u001b[0m                         exclude_domains\u001b[38;5;241m=\u001b[39mexclude_domains,\n\u001b[0;32m    105\u001b[0m                         include_answer\u001b[38;5;241m=\u001b[39minclude_answer,\n\u001b[0;32m    106\u001b[0m                         include_raw_content\u001b[38;5;241m=\u001b[39minclude_raw_content,\n\u001b[0;32m    107\u001b[0m                         include_images\u001b[38;5;241m=\u001b[39minclude_images,\n\u001b[0;32m    108\u001b[0m                         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    109\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    110\u001b[0m                         )\n\u001b[0;32m    112\u001b[0m     tavily_results \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m    114\u001b[0m     response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tavily_results\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:75\u001b[0m, in \u001b[0;36mTavilyClient._search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageLimitExceededError(detail)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidAPIKeyError()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m: The provided API key is invalid."
     ]
    }
   ],
   "source": [
    "# Let the model make as many steps of (parallel) tool calling, retrieval, self-reflection, before answering\n",
    "\n",
    "step = 0\n",
    "\n",
    "while response.tool_calls:\n",
    "    print(f\"\\n\\n\\033[1;34m == STEP {step+1} \\033[0m\\n\")  # Blue bold for step header\n",
    "\n",
    "    print(f\"\\033[1;32mThe plan is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")  # Green bold for \"The plan is\" and bold for the plan text\n",
    "\n",
    "    # Tool calls suggested by the model\n",
    "    print(f\"\\033[1;33mTool calls suggested by the model:\\033[0m\")  # Yellow bold for tool calls header\n",
    "    display_tool_calls(response.tool_calls)\n",
    "\n",
    "    # Execute the tool calls\n",
    "    tool_results = []\n",
    "    for tool_call in response.tool_calls:\n",
    "        outputs = functions_map[tool_call.name](**tool_call.parameters)\n",
    "        tool_result = {\"call\": tool_call, \"outputs\": outputs}\n",
    "        tool_results.append(tool_result)\n",
    "        # print(\"tool_results: \", tool_result['outputs'])\n",
    "        print(f\"\\033[1;35mtool_results:\\033[0m \\033[1m{tool_result['outputs']}\\033[0m\")  # Magenta bold for \"tool_results\" and bold for outputs\n",
    "\n",
    "    # call chat again with tool results\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        preamble=preamble,\n",
    "        message=\"\",\n",
    "        chat_history=response.chat_history,\n",
    "        tools=[web_search_tool,python_interpreter_tool],\n",
    "        tool_results=tool_results,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    step+=1\n",
    "\n",
    "# print final answer\n",
    "print(f\"\\n\\n \\033[1;32mThe final answer is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")  # Green bold for \"The plan is\" and bold for the plan text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Better Agentic workflow with weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSpotifyAgent(weave.Model):\n",
    "    preamble: str\n",
    "    tools: list\n",
    "    temperature: float = 0.0\n",
    "    model: str = \"command-r-plus-08-2024\"\n",
    "    max_steps: int = 10\n",
    "    debug: bool = False\n",
    "\n",
    "    @weave.op()\n",
    "    def run_spotify_agent(self, query: str) -> str:\n",
    "        # Use LLM for planning\n",
    "        response = co.chat(\n",
    "            model=self.model,\n",
    "            preamble=self.preamble,\n",
    "            message=query,\n",
    "            tools=self.tools,\n",
    "            temperature=self.temperature,\n",
    "            prompt_truncation=\"OFF\"\n",
    "        )\n",
    "\n",
    "        step = 0\n",
    "        while response.tool_calls:\n",
    "            if self.debug:\n",
    "                print(f\"\\n\\n\\033[1;34m == STEP {step+1} \\033[0m\\n\")\n",
    "                print(f\"\\033[1;32mThe plan is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")\n",
    "                print(f\"\\033[1;33mTool calls suggested by the model:\\033[0m\")\n",
    "                display_tool_calls(response.tool_calls)\n",
    "\n",
    "            tool_results = []\n",
    "            for tool_call in response.tool_calls:\n",
    "                outputs = functions_map[tool_call.name](**tool_call.parameters)\n",
    "                tool_result = {\"call\": tool_call, \"outputs\": outputs}\n",
    "                tool_results.append(tool_result)\n",
    "                if self.debug:\n",
    "                  print(f\"\\033[1;35mtool_results:\\033[0m \\033[1m{tool_result['outputs']}\\033[0m\")\n",
    "\n",
    "            response = co.chat(\n",
    "                model=self.model,\n",
    "                preamble=self.preamble,\n",
    "                message=\"\",\n",
    "                chat_history=response.chat_history,\n",
    "                tools=self.tools,\n",
    "                tool_results=tool_results,\n",
    "                temperature=self.temperature,\n",
    "            )\n",
    "            step+=1\n",
    "\n",
    "            if step >= self.max_steps:\n",
    "                print(f\"Could not find the answer in {self.max_steps} steps.\")\n",
    "                print(f\"The final response before ending execution: {response.text}\")\n",
    "                return f\"[MAX STEP REACHED]: {response.text}\"\n",
    "\n",
    "        if self.debug:\n",
    "          print(f\"\\n\\n \\033[1;32mThe final answer is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def infer(self, query: str) -> str:\n",
    "        return self.run_spotify_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble=\"\"\"You have access to to the internet.\n",
    "You also have access to a dataset with information about Spotify songs from the past 10 years, located at ./spotify_dataset.csv.\n",
    "Remember to inspect the dataset to understand its structure before querying it.\n",
    "Use the dataset when you can. Otherwise use the internet.\n",
    "\"\"\"\n",
    "\n",
    "spotify_agent = SimpleSpotifyAgent(\n",
    "    preamble=preamble,\n",
    "    tools=[web_search_tool, python_interpreter_tool],\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1;34m == STEP 1 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mI will first check the dataset to see if it contains information about the top 3 most streamed songs on Spotify in 2023. If it does, I will then find the age and citizenship of the artists of those songs.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "python_interpreter with this code:\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"spotify_dataset.csv\")\n",
      "  \n",
      "  # Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\n",
      "  if \"2023\" in df[\"year\"]: \n",
      "      print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "  else: \n",
      "      print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "\u001b[1;35mtool_results:\u001b[0m \u001b[1m[{'executed_code': 'import pandas as pd\\n\\ndf = pd.read_csv(\"spotify_dataset.csv\")\\n\\n# Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\\nif \"2023\" in df[\"year\"]: \\n    print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\\nelse: \\n    print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")', 'error': \"'year'\"}]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;34m == STEP 2 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mThe dataset does not contain information about the top 3 most streamed songs on Spotify in 2023. I will now search the web for this information.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "web_search\n",
      "{'query': 'top 3 most streamed songs on Spotify in 2023'}\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f0-f66b-7210-8133-d5784e36b102\n"
     ]
    },
    {
     "ename": "InvalidAPIKeyError",
     "evalue": "The provided API key is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mspotify_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms the age and citizenship of the artists who had the top 3 most streamed songs on Spotify in 2023?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:361\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:237\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:235\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_async()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     handle_exception(e)\n",
      "Cell \u001b[1;32mIn[62], line 60\u001b[0m, in \u001b[0;36mSimpleSpotifyAgent.infer\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop()\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_spotify_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:361\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:237\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:235\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_async()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     handle_exception(e)\n",
      "Cell \u001b[1;32mIn[62], line 31\u001b[0m, in \u001b[0;36mSimpleSpotifyAgent.run_spotify_agent\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m     29\u001b[0m tool_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m functions_map[tool_call\u001b[38;5;241m.\u001b[39mname](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_call\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m     32\u001b[0m     tool_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_call, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs}\n\u001b[0;32m     33\u001b[0m     tool_results\u001b[38;5;241m.\u001b[39mappend(tool_result)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:361\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:237\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:235\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_async()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     handle_exception(e)\n",
      "Cell \u001b[1;32mIn[48], line 4\u001b[0m, in \u001b[0;36mweb_search\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop() \u001b[38;5;66;03m# < ----- notice\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweb_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mtavily_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:98\u001b[0m, in \u001b[0;36mTavilyClient.search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     81\u001b[0m             query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     82\u001b[0m             search_depth: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvanced\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     93\u001b[0m             ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Combined search method.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search(query,\n\u001b[0;32m     99\u001b[0m                         search_depth\u001b[38;5;241m=\u001b[39msearch_depth,\n\u001b[0;32m    100\u001b[0m                         topic\u001b[38;5;241m=\u001b[39mtopic,\n\u001b[0;32m    101\u001b[0m                         days\u001b[38;5;241m=\u001b[39mdays,\n\u001b[0;32m    102\u001b[0m                         max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[0;32m    103\u001b[0m                         include_domains\u001b[38;5;241m=\u001b[39minclude_domains,\n\u001b[0;32m    104\u001b[0m                         exclude_domains\u001b[38;5;241m=\u001b[39mexclude_domains,\n\u001b[0;32m    105\u001b[0m                         include_answer\u001b[38;5;241m=\u001b[39minclude_answer,\n\u001b[0;32m    106\u001b[0m                         include_raw_content\u001b[38;5;241m=\u001b[39minclude_raw_content,\n\u001b[0;32m    107\u001b[0m                         include_images\u001b[38;5;241m=\u001b[39minclude_images,\n\u001b[0;32m    108\u001b[0m                         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    109\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    110\u001b[0m                         )\n\u001b[0;32m    112\u001b[0m     tavily_results \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m    114\u001b[0m     response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tavily_results\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:75\u001b[0m, in \u001b[0;36mTavilyClient._search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageLimitExceededError(detail)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidAPIKeyError()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m: The provided API key is invalid."
     ]
    }
   ],
   "source": [
    "output = spotify_agent.infer(\n",
    "    \"What's the age and citizenship of the artists who had the top 3 most streamed songs on Spotify in 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1;34m == STEP 1 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mI will first inspect the dataset to see if it contains a column for danceability. If it does, I will then write and execute Python code to find the most danceable song on Spotify in 2023.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "python_interpreter with this code:\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"spotify_dataset.csv\")\n",
      "  \n",
      "  # Inspect the dataset\n",
      "  print(df.info())\n",
      "\u001b[1;35mtool_results:\u001b[0m \u001b[1m[{'console_output': \"<class 'pandas.core.frame.DataFrame'>\\nRangeIndex: 114000 entries, 0 to 113999\\nData columns (total 21 columns):\\n #   Column            Non-Null Count   Dtype  \\n---  ------            --------------   -----  \\n 0   Unnamed: 0        114000 non-null  int64  \\n 1   track_id          114000 non-null  object \\n 2   artists           113999 non-null  object \\n 3   album_name        113999 non-null  object \\n 4   track_name        113999 non-null  object \\n 5   popularity        114000 non-null  int64  \\n 6   duration_ms       114000 non-null  int64  \\n 7   explicit          114000 non-null  bool   \\n 8   danceability      114000 non-null  float64\\n 9   energy            114000 non-null  float64\\n 10  key               114000 non-null  int64  \\n 11  loudness          114000 non-null  float64\\n 12  mode              114000 non-null  int64  \\n 13  speechiness       114000 non-null  float64\\n 14  acousticness      114000 non-null  float64\\n 15  instrumentalness  114000 non-null  float64\\n 16  liveness          114000 non-null  float64\\n 17  valence           114000 non-null  float64\\n 18  tempo             114000 non-null  float64\\n 19  time_signature    114000 non-null  int64  \\n 20  track_genre       114000 non-null  object \\ndtypes: bool(1), float64(9), int64(6), object(5)\\nmemory usage: 17.5+ MB\\nNone\\n\", 'executed_code': 'import pandas as pd\\r\\n\\r\\ndf = pd.read_csv(\"spotify_dataset.csv\")\\r\\n\\r\\n# Inspect the dataset\\r\\nprint(df.info())'}]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1;34m == STEP 2 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mThe dataset contains a column for danceability. I will now write and execute Python code to find the most danceable song on Spotify in 2023.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "python_interpreter with this code:\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"spotify_dataset.csv\")\n",
      "  \n",
      "  # Filter for songs released in 2023\n",
      "  df_2023 = df[df[\"track_name\"].str.contains(\"2023\", na=False)]\n",
      "  \n",
      "  # Find the most danceable song\n",
      "  most_danceable_song = df_2023[df_2023[\"danceability\"] == df_2023[\"danceability\"].max()][[\"track_name\", \"artists\"]].values.tolist()\n",
      "  \n",
      "  print(f\"Most danceable song on Spotify in 2023:\\n{most_danceable_song}\")\n",
      "\u001b[1;35mtool_results:\u001b[0m \u001b[1m[{'console_output': 'Most danceable song on Spotify in 2023:\\n[[\"It\\'s Time to Wake Up 2023\", \\'La Femme\\']]\\n', 'executed_code': 'import pandas as pd\\r\\n\\r\\ndf = pd.read_csv(\"spotify_dataset.csv\")\\r\\n\\r\\n# Filter for songs released in 2023\\r\\ndf_2023 = df[df[\"track_name\"].str.contains(\"2023\", na=False)]\\r\\n\\r\\n# Find the most danceable song\\r\\nmost_danceable_song = df_2023[df_2023[\"danceability\"] == df_2023[\"danceability\"].max()][[\"track_name\", \"artists\"]].values.tolist()\\r\\n\\r\\nprint(f\"Most danceable song on Spotify in 2023:\\\\n{most_danceable_song}\")'}]\u001b[0m\n",
      "\n",
      "\n",
      " \u001b[1;32mThe final answer is:\u001b[0m \u001b[1mThe most danceable song on Spotify in 2023 is 'It's Time to Wake Up 2023' by La Femme.\u001b[0m\n",
      "\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3f1-6d34-7931-abf5-c7b7ea514bee\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The most danceable song on Spotify in 2023 is 'It's Time to Wake Up 2023' by La Femme.\""
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_agent.infer(\n",
    "    \"What's the most danceable song on Spotify in 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can evaluate:**\n",
    "\n",
    "how accurately is LLM coming up with function name/function arguments?\n",
    "- Is function calling happening correctly?\n",
    "- Is the final answer correct based on the retrieved content from web?\n",
    "- Is the final answer is based off tool x's result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>...</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows 칑 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                track_id                 artists  \\\n",
       "0           0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                          album_name  \\\n",
       "0                                             Comedy   \n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy          73       230666     False   \n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "\n",
       "   danceability  energy  ...  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610  ...    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660  ...   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590  ...    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596  ...   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430  ...    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature  track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4     acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4     acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4     acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3     acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4     acoustic  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt: open spotify_dataset and print head\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./spotify_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游닍 Published to https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/weave/objects/response_eval/versions/SuFwcpK5uXWU2Kk36YDlDigAUX0TWiXpSUB7Qfriq5E\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectRef(entity='dastech1998-ozyegin-university', project='cohere-weave-project', name='response_eval', digest='SuFwcpK5uXWU2Kk36YDlDigAUX0TWiXpSUB7Qfriq5E', extra=())"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initial e2e evaluation set based off the dataset\n",
    "dataset_based_eval = [\n",
    "    {\"query\": \"Which song is the most danceable on Spotify in 2023?\", \"ground_truth\": \"Gol Bolinha, Gol Quadrado 2\"},\n",
    "    {\"query\": \"Which song has the most number of artists collaborating?\", \"ground_truth\": \"Los del Espacio\"},\n",
    "    {\"query\": \"How many song did Taylor Swift release in the last 5 years?\", \"ground_truth\": \"28\"},\n",
    "]\n",
    "\n",
    "e2e_eval_set = weave.Dataset(name=\"response_eval\", rows=dataset_based_eval)\n",
    "weave.publish(e2e_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'released_year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19268\\498196247.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# wrote quick one liners to get the answer; an example below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreleased_year\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m2023\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdanceability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midxmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'released_year'"
     ]
    }
   ],
   "source": [
    "# wrote quick one liners to get the answer; an example below\n",
    "df.loc[df.loc[df.released_year==2023].danceability.idxmax()].track_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "## Correctness scorer\n",
    "llm_correctness_judge = \"\"\"You are responsible to evaluate the response of a system against some groud truth.\n",
    "Return your judgement in a valid JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"judgement\": \"CORRECT\" | \"INCORRECT\",\n",
    "  \"reasoning\": \"Reasoning for your judgement\",\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "llm_correctness_judge_message = \"\"\"The provided ground truth and generated response are:\n",
    "\n",
    "GROUND TRUTH:\n",
    "{ground_truth}\n",
    "\n",
    "GENERATED RESPONSE:\n",
    "{generated_text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def correctness_evaluator(ground_truth: str, model_output):\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        preamble=llm_correctness_judge,\n",
    "        message=llm_correctness_judge_message.format(ground_truth=ground_truth, generated_text=model_output),\n",
    "        temperature=0,\n",
    "        prompt_truncation=\"OFF\"\n",
    "    )\n",
    "\n",
    "    # ideally run with retries or use structured output parsing\n",
    "    try:\n",
    "        eval = json.loads(response.text)\n",
    "        return {\n",
    "            \"score\": eval[\"judgement\"] == \"CORRECT\",\n",
    "            \"reasoning\": eval[\"reasoning\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"score\": False,\n",
    "            \"reasoning\": response.text\n",
    "        }\n",
    "\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"Response Evaluation\",\n",
    "    dataset=e2e_eval_set,\n",
    "    scorers=[correctness_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<unknown>, line 622)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:3577\u001b[0m in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[0;32mIn[70], line 1\u001b[0m\n    asyncio.run(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:30\u001b[0m in \u001b[0;35mrun\u001b[0m\n    return loop.run_until_complete(task)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nest_asyncio.py:98\u001b[0m in \u001b[0;35mrun_until_complete\u001b[0m\n    return f.result()\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\futures.py:201\u001b[0m in \u001b[0;35mresult\u001b[0m\n    raise self._exception.with_traceback(self._exception_tb)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\tasks.py:232\u001b[0m in \u001b[0;35m__step\u001b[0m\n    result = coro.send(None)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:347\u001b[0m in \u001b[0;35mwrapper\u001b[0m\n    call = _create_call(wrapper, *args, **kwargs)  # type: ignore\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:174\u001b[0m in \u001b[0;35m_create_call\u001b[0m\n    return client.create_call(\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\trace_sentry.py:211\u001b[0m in \u001b[0;35mwrapper\u001b[0m\n    return func(*args, **kwargs)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\weave_client.py:504\u001b[0m in \u001b[0;35mcreate_call\u001b[0m\n    op_def_ref = self._save_op(unbound_op)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\weave_client.py:874\u001b[0m in \u001b[0;35m_save_op\u001b[0m\n    return self._save_and_attach_ref(op, name)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\weave_client.py:880\u001b[0m in \u001b[0;35m_save_and_attach_ref\u001b[0m\n    op_def_ref = self._save_object_basic(op, name)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\weave_client.py:756\u001b[0m in \u001b[0;35m_save_object_basic\u001b[0m\n    json_val = to_json(val, self._project_id(), self.server)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\serialize.py:34\u001b[0m in \u001b[0;35mto_json\u001b[0m\n    encoded = custom_objs.encode_custom_obj(obj)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\custom_objs.py:104\u001b[0m in \u001b[0;35mencode_custom_obj\u001b[0m\n    serializer.save(obj, art, \"obj\")\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op_type.py:472\u001b[0m in \u001b[0;35msave_instance\u001b[0m\n    result = get_code_deps(obj.resolve_fn, artifact)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op_type.py:361\u001b[0m in \u001b[0;35mget_code_deps\u001b[0m\n    result = get_code_deps(var_value, artifact, depth + 1)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op_type.py:322\u001b[0m in \u001b[0;35mget_code_deps\u001b[0m\n    source = get_source_or_fallback(fn)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op_type.py:283\u001b[0m in \u001b[0;35mget_source_or_fallback\u001b[0m\n    return get_source_notebook_safe(fn)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op_type.py:202\u001b[0m in \u001b[0;35mget_source_notebook_safe\u001b[0m\n    src = get_class_source(fn)\u001b[0m\n",
      "\u001b[0m  File \u001b[0;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\ipython.py:46\u001b[0m in \u001b[0;35mget_class_source\u001b[0m\n    tree = ast.parse(notebook_source)\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ast.py:50\u001b[1;36m in \u001b[1;35mparse\u001b[1;36m\n\u001b[1;33m    return compile(source, filename, mode, flags,\u001b[1;36m\n",
      "\u001b[1;36m  File \u001b[1;32m<unknown>:622\u001b[1;36m\u001b[0m\n\u001b[1;33m    //weave_client =\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "asyncio.run(\n",
    "    evaluation.evaluate(\n",
    "        SimpleSpotifyAgent(\n",
    "          preamble=preamble,\n",
    "          tools=[web_search_tool,python_interpreter_tool],\n",
    "          debug=False,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build offline trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GROUND_TRUTH_TRAJECTORY\n",
    "# @markdown The ground truth trajectory can be created manually or by including the trajectories of the best agentic runs.\n",
    "\n",
    "GROUND_TRUTH_TRAJECTORY = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dc5-74f0-9f41-38eca92ca325'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dc5-74f0-9f41-38f4177a0c9a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dc8-7693-9acc-c949ab590947'),\n",
    "            (2, 'python_interpreter', '01918f12-7d98-73a0-ae7e-ce6761b838db'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7db3-7d31-a25e-da2f9fb17f3a'),\n",
    "            (2, 'python_interpreter', '01918f12-999a-7b20-b5cd-17af9b85c70e'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-99a5-7ec2-a684-632f2cfc2ab9')\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dca-78e2-96c7-d8f843776ec1'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dcb-7231-ba86-f870c5dfe992'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dcd-7f61-9be2-b94e051862a8'),\n",
    "            (2, 'python_interpreter', '01918f12-7e4a-7dc2-9943-47a0ffbb9d4a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7e54-7930-9695-ecf98cfce0e0'),\n",
    "            (2, 'python_interpreter', '01918f12-9968-7371-bfa1-0b8e55888cc6'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-9973-7da0-a50e-4fcfec42e3aa')\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dd1-7e12-ad34-042fec1bba62'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dd2-7e63-9bba-5218ec198526'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dd4-74f3-8ba2-bdb3c0b7e1ac'),\n",
    "            (2, 'python_interpreter', '01918f12-7e01-76b2-b3b8-e6cdd1b6ad1a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7e17-7542-9697-82301d29d4c9'),\n",
    "            (2, 'python_interpreter', '01918f12-95d3-7173-b518-f747ef2bae21'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-95e2-7792-a783-3a0642673af4')\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eval_calls = weave_client.call(\"01919262-59bd-7e22-b7eb-8817d3ca1d02\")\n",
    "\n",
    "eval_call_trace_ids = {}\n",
    "for call in _eval_calls.children():\n",
    "    if call.op_name.split(\"/\")[-1].split(\":\")[0] == \"Evaluation.predict_and_score\":\n",
    "        for child in call.children():\n",
    "            if child.op_name.split(\"/\")[-1].split(\":\")[0] == \"SimpleSpotifyAgent.infer\":\n",
    "                eval_call_trace_ids[child.id] = child\n",
    "\n",
    "eval_call_trace_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to recursively build a trajectory\n",
    "def traverse_children(call, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively traverse the children of a call and collect tuples of (depth, call.op_name, call.id).\n",
    "    \"\"\"\n",
    "    # Start with the current call's tuple\n",
    "    trajectory = [(depth, call.op_name.split(\"/\")[-1].split(\":\")[0], call.id)]\n",
    "\n",
    "    # Recurse into each child and extend the trajectory\n",
    "    for child in call.children():\n",
    "        trajectory.extend(traverse_children(child, depth + 1))\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trace_id = \"01919262-5ab5-7521-94e9-6efa0ae9c794\"\n",
    "root_call = eval_call_trace_ids[trace_id]\n",
    "\n",
    "trajectory = traverse_children(root_call)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories = []\n",
    "\n",
    "for trace_id, _ in eval_call_trace_ids.items():\n",
    "    root_call = eval_call_trace_ids[trace_id]\n",
    "    trajectory = traverse_children(root_call)\n",
    "    all_trajectories.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def exact_match(model_output: list[tuple], trajectory: list[tuple]) -> float:\n",
    "    correct = sum(1 for p, g in zip(model_output, trajectory) if p[0] == g[0] and p[1] == g[1])\n",
    "    return correct / len(trajectory)\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def levenshtein_distance(model_output: list[tuple], trajectory: list[tuple]) -> float:\n",
    "    seq1 = [(p[0], p[1]) for p in model_output]\n",
    "    seq2 = [(g[0], g[1]) for g in trajectory]\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    return matcher.ratio()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_predicted_trajectory(id: int) -> list[tuple]:\n",
    "    return all_trajectories[id]\n",
    "\n",
    "\n",
    "trajectory_evaluation = weave.Evaluation(\n",
    "    name=\"Trajectory Evaluation\",\n",
    "    dataset=GROUND_TRUTH_TRAJECTORY,\n",
    "    scorers=[exact_match, levenshtein_distance],\n",
    ")\n",
    "\n",
    "asyncio.run(\n",
    "    trajectory_evaluation.evaluate(get_predicted_trajectory)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Factfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## factfulness scorer\n",
    "llm_factful_judge = \"\"\"You are an expert checker of facts. Given the context you can find if the generated text is based out of the context.\n",
    "If the generated text is coming from the provided context return \"CORRECT\", otherwise return \"INCORRECT\".\n",
    "\n",
    "Return your judgement in a valid JSON format:\n",
    "\n",
    "{\n",
    "  \"judgement\": \"CORRECT\" | \"INCORRECT\",\n",
    "  \"reasoning\": \"Reasoning for your judgement\",\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "llm_factful_judge_message = \"\"\"The provided context and generated text are:\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "GENERATED TEXT:\n",
    "{generated_text}\n",
    "\"\"\"\n",
    "\n",
    "@weave.op()\n",
    "def factfulness_evaluator(model_output: dict):\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        preamble=llm_factful_judge,\n",
    "        message=llm_factful_judge_message.format(\n",
    "            context=\" \\n\".join(model_output[\"context\"]), generated_text=model_output[\"generated_text\"]\n",
    "        ),\n",
    "        temperature=0,\n",
    "        prompt_truncation=\"OFF\"\n",
    "    )\n",
    "\n",
    "    # ideally run with retries or use structured output parsing\n",
    "    try:\n",
    "        eval = json.loads(response.text)\n",
    "        return {\n",
    "            \"score\": eval[\"judgement\"] == \"CORRECT\",\n",
    "            \"reasoning\": eval[\"reasoning\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"score\": False,\n",
    "            \"reasoning\": response.text\n",
    "        }\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_context_and_answer(id: int) -> list[str]:\n",
    "    # we get the context from the 2nd last component of the trajectory\n",
    "    trajectory = all_trajectories[id]\n",
    "\n",
    "    if trajectory[-2][1] == \"python_interpreter\":\n",
    "        context = [weave_client.call(trajectory[-2][-1]).output[0][\"console_output\"]]\n",
    "\n",
    "    if trajectory[-2][1] == \"web_search\":\n",
    "        context = [web_result[\"content\"] for web_result in weave_client.call(trajectory[-2][-1]).output]\n",
    "\n",
    "    generated_text = weave.ref(weave_client.call(trajectory[-1][-1]).output).get().text\n",
    "\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"generated_text\": generated_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factfulness_evaluation = weave.Evaluation(\n",
    "    name=\"Factfulness Evaluation\",\n",
    "    dataset=GROUND_TRUTH_TRAJECTORY,\n",
    "    scorers=[factfulness_evaluator],\n",
    ")\n",
    "\n",
    "asyncio.run(\n",
    "    factfulness_evaluation.evaluate(get_context_and_answer)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
