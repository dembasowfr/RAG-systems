{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install cohere weave --q --disable-pip-version-check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weave # < --- notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tool_calls(tool_calls):\n",
    "    # Determine the message based on the number of tool calls\n",
    "    if len(tool_calls) > 1:\n",
    "        print(\"The model suggests making Parallel Queries:\")\n",
    "    else:\n",
    "        print(\"The model suggests making a single tool call:\")\n",
    "\n",
    "    for i, tool_call in enumerate(tool_calls):\n",
    "        # If there's more than one tool call, separate each with a header\n",
    "        if len(tool_calls) > 1:\n",
    "            print(f\"== Parallel Tool Call #{i+1}\")\n",
    "\n",
    "        # Print the tool call name and \"with this code:\" on the same line\n",
    "        if tool_call.name == 'python_interpreter':\n",
    "            print(f\"{tool_call.name} with this code:\")\n",
    "            code = tool_call.parameters.get('code', '')\n",
    "            print(\"\\n\".join(f\"  {line}\" for line_num, line in enumerate(code.splitlines())))\n",
    "        else:\n",
    "            # For non-python_interpreter tool calls, just print the parameters\n",
    "            print(f\"{tool_call.name}\")\n",
    "            print(f\"{tool_call.parameters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Tools for Agentic RAG: a web search engine and a spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. A web search engine, accessible through an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tavily-python --q --disable-pip-version-check\n",
    "from tavily import TavilyClient\n",
    "tavily_client = TavilyClient(api_key=\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a web search engine\n",
    "@weave.op() # < ----- notice\n",
    "def web_search(query: str) -> list[dict]:\n",
    "  response = tavily_client.search(query, max_results=3)['results']\n",
    "  return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the LLM is equipped with a description of the web search engine\n",
    "web_search_tool = {\n",
    "  \"name\": \"web_search\",\n",
    "  \"description\": \"Returns a list of relevant document snippets for a textual query retrieved from the internet\",\n",
    "  \"parameter_definitions\": {\n",
    "    \"query\": {\n",
    "      \"description\": \"Query to search the internet with\",\n",
    "      \"type\": \"str\",\n",
    "      \"required\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. A spreadsheet, accessible through a Python interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here's a python console, which can be used to access the spreadsheet, but also more generally to code and plot stuff\n",
    "import io, contextlib\n",
    "\n",
    "\n",
    "@weave.op() # < --- notice\n",
    "def python_interpreter(code: str) -> list[dict]:\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        # Redirect stdout to capture print statements\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            exec(code, globals())\n",
    "    except Exception as e:\n",
    "        return [{\n",
    "            \"executed_code\": code,\n",
    "            \"error\": str(e)\n",
    "\n",
    "        }]\n",
    "    # Get the output value\n",
    "    return [{\n",
    "  \t\t\"console_output\": output.getvalue(),\n",
    "      \"executed_code\": code\n",
    "  \t}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the LLM is equipped with a description of a python console\n",
    "python_interpreter_tool = {\n",
    "  \"name\": \"python_interpreter\",\n",
    "  \"description\": \"Executes python code and returns the result. The code runs in a static sandbox without internet access and without interactive mode, so print output or save output to a file.\",\n",
    "  \"parameter_definitions\": {\n",
    "    \"code\": {\n",
    "      \"description\": \"Python code to execute\",\n",
    "      \"type\": \"str\",\n",
    "      \"required\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions_map = {\n",
    "    \"web_search\": web_search,\n",
    "    \"python_interpreter\": python_interpreter,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try Agentic RAG with a Cohere model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# < --- notice\n",
    "import cohere\n",
    "from weave.integrations.cohere import cohere_patcher\n",
    "\n",
    "cohere_patcher.attempt_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\almud\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Ensure the dotenv usage:\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Create cohere model and intergrate Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cohere\n",
    "\n",
    "# Integrate cohere with weave\n",
    "from weave.integrations.cohere import cohere_patcher\n",
    "\n",
    "cohere_patcher.attempt_patch()\n",
    "\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Cohere client using an environment variable for the API key\n",
    "api_key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "# Check if the .env file is loaded\n",
    "#print(f\"Cohere API Key: {api_key}\") \n",
    "\n",
    "\n",
    "co = cohere.Client(api_key=api_key)\n",
    "\n",
    "\n",
    "model = \"command-r-plus-08-2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Let's look at a complex user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble=\"\"\"You have access to to the internet.\n",
    "You also have access to a dataset with information about Spotify songs from the past 10 years, located at ./spotify_dataset.csv.\n",
    "Remember to inspect the dataset to understand its structure before querying it.\n",
    "Use the dataset when you can. Otherwise use the internet.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What's the age and citizenship of the artists who had the top 3 most streamed songs on Spotify in 2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Get the model plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weave_client = weave.init(\"cohere-weave-project\") # < --- notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3d6-6848-79e0-9d29-353b20817ed7\n",
      "I will first check the dataset to see if it contains information about the top 3 most streamed songs on Spotify in 2023. If it does, I will then find the age and citizenship of the artists of those songs.\n"
     ]
    }
   ],
   "source": [
    "response = co.chat(\n",
    "    model=model,\n",
    "    preamble=preamble,\n",
    "    message=message,\n",
    "    tools=[web_search_tool, python_interpreter_tool],\n",
    "    temperature=0,\n",
    "    prompt_truncation=\"OFF\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. RAG System Itarations for final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1;34m == STEP 1 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mI will first check the dataset to see if it contains information about the top 3 most streamed songs on Spotify in 2023. If it does, I will then find the age and citizenship of the artists of those songs.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "python_interpreter with this code:\n",
      "  import pandas as pd\n",
      "  \n",
      "  df = pd.read_csv(\"spotify_dataset.csv\")\n",
      "  \n",
      "  # Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\n",
      "  if \"2023\" in df[\"year\"]: \n",
      "      print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "  else:\n",
      "      print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3d6-894b-7570-9a6b-49b0c0d428e5\n",
      "\u001b[1;35mtool_results:\u001b[0m \u001b[1m[{'executed_code': 'import pandas as pd\\n\\ndf = pd.read_csv(\"spotify_dataset.csv\")\\n\\n# Check if the dataset contains information about the top 3 most streamed songs on Spotify in 2023\\nif \"2023\" in df[\"year\"]: \\n    print(f\"The dataset contains information about the top 3 most streamed songs on Spotify in 2023\")\\nelse:\\n    print(f\"The dataset does not contain information about the top 3 most streamed songs on Spotify in 2023\")', 'error': \"'year'\"}]\u001b[0m\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3d6-9488-7b60-a66e-d1bdb60f48e9\n",
      "\n",
      "\n",
      "\u001b[1;34m == STEP 2 \u001b[0m\n",
      "\n",
      "\u001b[1;32mThe plan is:\u001b[0m \u001b[1mThe dataset does not contain information about the top 3 most streamed songs on Spotify in 2023. I will now search the web for this information.\u001b[0m\n",
      "\n",
      "\u001b[1;33mTool calls suggested by the model:\u001b[0m\n",
      "The model suggests making a single tool call:\n",
      "web_search\n",
      "{'query': 'top 3 most streamed songs on Spotify in 2023'}\n",
      "游꼴 https://wandb.ai/dastech1998-ozyegin-university/cohere-weave-project/r/call/0191a3d6-b473-7900-9fe0-582c45e54634\n"
     ]
    },
    {
     "ename": "InvalidAPIKeyError",
     "evalue": "The provided API key is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m tool_results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls:\n\u001b[1;32m---> 15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m functions_map[tool_call\u001b[38;5;241m.\u001b[39mname](\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_call\u001b[38;5;241m.\u001b[39mparameters)\n\u001b[0;32m     16\u001b[0m     tool_result \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_call, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs}\n\u001b[0;32m     17\u001b[0m     tool_results\u001b[38;5;241m.\u001b[39mappend(tool_result)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:361\u001b[0m, in \u001b[0;36mop.<locals>.op_deco.<locals>.create_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    360\u001b[0m call \u001b[38;5;241m=\u001b[39m _create_call(wrapper, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m res, _ \u001b[38;5;241m=\u001b[39m _execute_call(wrapper, call, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:237\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 237\u001b[0m     \u001b[43mhandle_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m process(res)\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\weave\\trace\\op.py:235\u001b[0m, in \u001b[0;36m_execute_call\u001b[1;34m(__op, call, __should_raise, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_async()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    237\u001b[0m     handle_exception(e)\n",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m, in \u001b[0;36mweb_search\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;129m@weave\u001b[39m\u001b[38;5;241m.\u001b[39mop() \u001b[38;5;66;03m# < ----- notice\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweb_search\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m----> 4\u001b[0m   response \u001b[38;5;241m=\u001b[39m \u001b[43mtavily_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:98\u001b[0m, in \u001b[0;36mTavilyClient.search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     81\u001b[0m             query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     82\u001b[0m             search_depth: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madvanced\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbasic\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     93\u001b[0m             ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m     94\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    Combined search method.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     response_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_search(query,\n\u001b[0;32m     99\u001b[0m                         search_depth\u001b[38;5;241m=\u001b[39msearch_depth,\n\u001b[0;32m    100\u001b[0m                         topic\u001b[38;5;241m=\u001b[39mtopic,\n\u001b[0;32m    101\u001b[0m                         days\u001b[38;5;241m=\u001b[39mdays,\n\u001b[0;32m    102\u001b[0m                         max_results\u001b[38;5;241m=\u001b[39mmax_results,\n\u001b[0;32m    103\u001b[0m                         include_domains\u001b[38;5;241m=\u001b[39minclude_domains,\n\u001b[0;32m    104\u001b[0m                         exclude_domains\u001b[38;5;241m=\u001b[39mexclude_domains,\n\u001b[0;32m    105\u001b[0m                         include_answer\u001b[38;5;241m=\u001b[39minclude_answer,\n\u001b[0;32m    106\u001b[0m                         include_raw_content\u001b[38;5;241m=\u001b[39minclude_raw_content,\n\u001b[0;32m    107\u001b[0m                         include_images\u001b[38;5;241m=\u001b[39minclude_images,\n\u001b[0;32m    108\u001b[0m                         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m    109\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    110\u001b[0m                         )\n\u001b[0;32m    112\u001b[0m     tavily_results \u001b[38;5;241m=\u001b[39m response_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m    114\u001b[0m     response_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tavily_results\n",
      "File \u001b[1;32mc:\\Users\\almud\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tavily\\tavily.py:75\u001b[0m, in \u001b[0;36mTavilyClient._search\u001b[1;34m(self, query, search_depth, topic, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, use_cache)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UsageLimitExceededError(detail)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidAPIKeyError()\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
      "\u001b[1;31mInvalidAPIKeyError\u001b[0m: The provided API key is invalid."
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "while response.tool_calls:\n",
    "    print(f\"\\n\\n\\033[1;34m == STEP {step+1} \\033[0m\\n\")  # Blue bold for step header\n",
    "\n",
    "    print(f\"\\033[1;32mThe plan is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")  # Green bold for \"The plan is\" and bold for the plan text\n",
    "\n",
    "    # Tool calls suggested by the model\n",
    "    print(f\"\\033[1;33mTool calls suggested by the model:\\033[0m\")  # Yellow bold for tool calls header\n",
    "    display_tool_calls(response.tool_calls)\n",
    "\n",
    "    # Execute the tool calls\n",
    "    tool_results = []\n",
    "    for tool_call in response.tool_calls:\n",
    "        outputs = functions_map[tool_call.name](**tool_call.parameters)\n",
    "        tool_result = {\"call\": tool_call, \"outputs\": outputs}\n",
    "        tool_results.append(tool_result)\n",
    "        # print(\"tool_results: \", tool_result['outputs'])\n",
    "        print(f\"\\033[1;35mtool_results:\\033[0m \\033[1m{tool_result['outputs']}\\033[0m\")  # Magenta bold for \"tool_results\" and bold for outputs\n",
    "\n",
    "    # call chat again with tool results\n",
    "    response = co.chat(\n",
    "        model=model,\n",
    "        preamble=preamble,\n",
    "        message=\"\",\n",
    "        chat_history=response.chat_history,\n",
    "        tools=[web_search_tool,python_interpreter_tool],\n",
    "        tool_results=tool_results,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    step+=1\n",
    "\n",
    "# print final answer\n",
    "print(f\"\\n\\n \\033[1;32mThe final answer is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")  # Green bold for \"The plan is\" and bold for the plan text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Better Agentic workflow with weave.Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSpotifyAgent(weave.Model):\n",
    "    preamble: str\n",
    "    tools: list\n",
    "    temperature: float = 0.0\n",
    "    model: str = \"command-r-plus-08-2024\"\n",
    "    max_steps: int = 10\n",
    "    debug: bool = False\n",
    "\n",
    "    @weave.op()\n",
    "    def run_spotify_agent(self, query: str) -> str:\n",
    "        # Use LLM for planning\n",
    "        response = co.chat(\n",
    "            model=self.model,\n",
    "            preamble=self.preamble,\n",
    "            message=query,\n",
    "            tools=self.tools,\n",
    "            temperature=self.temperature,\n",
    "            prompt_truncation=\"OFF\"\n",
    "        )\n",
    "\n",
    "        step = 0\n",
    "        while response.tool_calls:\n",
    "            if self.debug:\n",
    "                print(f\"\\n\\n\\033[1;34m == STEP {step+1} \\033[0m\\n\")\n",
    "                print(f\"\\033[1;32mThe plan is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")\n",
    "                print(f\"\\033[1;33mTool calls suggested by the model:\\033[0m\")\n",
    "                display_tool_calls(response.tool_calls)\n",
    "\n",
    "            tool_results = []\n",
    "            for tool_call in response.tool_calls:\n",
    "                outputs = functions_map[tool_call.name](**tool_call.parameters)\n",
    "                tool_result = {\"call\": tool_call, \"outputs\": outputs}\n",
    "                tool_results.append(tool_result)\n",
    "                if self.debug:\n",
    "                  print(f\"\\033[1;35mtool_results:\\033[0m \\033[1m{tool_result['outputs']}\\033[0m\")\n",
    "\n",
    "            response = co.chat(\n",
    "                model=self.model,\n",
    "                preamble=self.preamble,\n",
    "                message=\"\",\n",
    "                chat_history=response.chat_history,\n",
    "                tools=self.tools,\n",
    "                tool_results=tool_results,\n",
    "                temperature=self.temperature,\n",
    "            )\n",
    "            step+=1\n",
    "\n",
    "            if step >= self.max_steps:\n",
    "                print(f\"Could not find the answer in {self.max_steps} steps.\")\n",
    "                print(f\"The final response before ending execution: {response.text}\")\n",
    "                return f\"[MAX STEP REACHED]: {response.text}\"\n",
    "\n",
    "        if self.debug:\n",
    "          print(f\"\\n\\n \\033[1;32mThe final answer is:\\033[0m \\033[1m{response.text}\\033[0m\\n\")\n",
    "\n",
    "        return response.text\n",
    "\n",
    "    @weave.op()\n",
    "    def infer(self, query: str) -> str:\n",
    "        return self.run_spotify_agent(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preamble=\"\"\"You have access to to the internet.\n",
    "You also have access to a dataset with information about Spotify songs from the past 10 years, located at ./spotify_dataset.csv.\n",
    "Remember to inspect the dataset to understand its structure before querying it.\n",
    "Use the dataset when you can. Otherwise use the internet.\n",
    "\"\"\"\n",
    "\n",
    "spotify_agent = SimpleSpotifyAgent(\n",
    "    preamble=preamble,\n",
    "    tools=[web_search_tool, python_interpreter_tool],\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = spotify_agent.infer(\n",
    "    \"What's the age and citizenship of the artists who had the top 3 most streamed songs on Spotify in 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_agent.infer(\n",
    "    \"What's the most danceable song on Spotify in 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can evaluate:**\n",
    "\n",
    "how accurately is LLM coming up with function name/function arguments?\n",
    "- Is function calling happening correctly?\n",
    "- Is the final answer correct based on the retrieved content from web?\n",
    "- Is the final answer is based off tool x's result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: open spotify_dataset and print head\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./spotify_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial e2e evaluation set based off the dataset\n",
    "dataset_based_eval = [\n",
    "    {\"query\": \"Which song is the most danceable on Spotify in 2023?\", \"ground_truth\": \"Gol Bolinha, Gol Quadrado 2\"},\n",
    "    {\"query\": \"Which song has the most number of artists collaborating?\", \"ground_truth\": \"Los del Espacio\"},\n",
    "    {\"query\": \"How many song did Taylor Swift release in the last 5 years?\", \"ground_truth\": \"28\"},\n",
    "]\n",
    "\n",
    "e2e_eval_set = weave.Dataset(name=\"response_eval\", rows=dataset_based_eval)\n",
    "weave.publish(e2e_eval_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrote quick one liners to get the answer; an example below\n",
    "df.loc[df.loc[df.released_year==2023].danceability.idxmax()].track_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "## Correctness scorer\n",
    "llm_correctness_judge = \"\"\"You are responsible to evaluate the response of a system against some groud truth.\n",
    "Return your judgement in a valid JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"judgement\": \"CORRECT\" | \"INCORRECT\",\n",
    "  \"reasoning\": \"Reasoning for your judgement\",\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "llm_correctness_judge_message = \"\"\"The provided ground truth and generated response are:\n",
    "\n",
    "GROUND TRUTH:\n",
    "{ground_truth}\n",
    "\n",
    "GENERATED RESPONSE:\n",
    "{generated_text}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def correctness_evaluator(ground_truth: str, model_output):\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        preamble=llm_correctness_judge,\n",
    "        message=llm_correctness_judge_message.format(ground_truth=ground_truth, generated_text=model_output),\n",
    "        temperature=0,\n",
    "        prompt_truncation=\"OFF\"\n",
    "    )\n",
    "\n",
    "    # ideally run with retries or use structured output parsing\n",
    "    try:\n",
    "        eval = json.loads(response.text)\n",
    "        return {\n",
    "            \"score\": eval[\"judgement\"] == \"CORRECT\",\n",
    "            \"reasoning\": eval[\"reasoning\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"score\": False,\n",
    "            \"reasoning\": response.text\n",
    "        }\n",
    "\n",
    "\n",
    "evaluation = weave.Evaluation(\n",
    "    name=\"Response Evaluation\",\n",
    "    dataset=e2e_eval_set,\n",
    "    scorers=[correctness_evaluator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(\n",
    "    evaluation.evaluate(\n",
    "        SimpleSpotifyAgent(\n",
    "          preamble=preamble,\n",
    "          tools=[web_search_tool,python_interpreter_tool],\n",
    "          debug=False,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build offline trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title GROUND_TRUTH_TRAJECTORY\n",
    "# @markdown The ground truth trajectory can be created manually or by including the trajectories of the best agentic runs.\n",
    "\n",
    "GROUND_TRUTH_TRAJECTORY = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dc5-74f0-9f41-38eca92ca325'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dc5-74f0-9f41-38f4177a0c9a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dc8-7693-9acc-c949ab590947'),\n",
    "            (2, 'python_interpreter', '01918f12-7d98-73a0-ae7e-ce6761b838db'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7db3-7d31-a25e-da2f9fb17f3a'),\n",
    "            (2, 'python_interpreter', '01918f12-999a-7b20-b5cd-17af9b85c70e'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-99a5-7ec2-a684-632f2cfc2ab9')\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dca-78e2-96c7-d8f843776ec1'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dcb-7231-ba86-f870c5dfe992'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dcd-7f61-9be2-b94e051862a8'),\n",
    "            (2, 'python_interpreter', '01918f12-7e4a-7dc2-9943-47a0ffbb9d4a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7e54-7930-9695-ecf98cfce0e0'),\n",
    "            (2, 'python_interpreter', '01918f12-9968-7371-bfa1-0b8e55888cc6'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-9973-7da0-a50e-4fcfec42e3aa')\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"trajectory\": [\n",
    "            (0, 'SimpleSpotifyAgent.infer', '01918f12-6dd1-7e12-ad34-042fec1bba62'),\n",
    "            (1, 'SimpleSpotifyAgent.run_spotify_agent', '01918f12-6dd2-7e63-9bba-5218ec198526'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-6dd4-74f3-8ba2-bdb3c0b7e1ac'),\n",
    "            (2, 'python_interpreter', '01918f12-7e01-76b2-b3b8-e6cdd1b6ad1a'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-7e17-7542-9697-82301d29d4c9'),\n",
    "            (2, 'python_interpreter', '01918f12-95d3-7173-b518-f747ef2bae21'),\n",
    "            (2, 'cohere.Client.chat', '01918f12-95e2-7792-a783-3a0642673af4')\n",
    "        ]\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_eval_calls = weave_client.call(\"01919262-59bd-7e22-b7eb-8817d3ca1d02\")\n",
    "\n",
    "eval_call_trace_ids = {}\n",
    "for call in _eval_calls.children():\n",
    "    if call.op_name.split(\"/\")[-1].split(\":\")[0] == \"Evaluation.predict_and_score\":\n",
    "        for child in call.children():\n",
    "            if child.op_name.split(\"/\")[-1].split(\":\")[0] == \"SimpleSpotifyAgent.infer\":\n",
    "                eval_call_trace_ids[child.id] = child\n",
    "\n",
    "eval_call_trace_ids.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to recursively build a trajectory\n",
    "def traverse_children(call, depth=0):\n",
    "    \"\"\"\n",
    "    Recursively traverse the children of a call and collect tuples of (depth, call.op_name, call.id).\n",
    "    \"\"\"\n",
    "    # Start with the current call's tuple\n",
    "    trajectory = [(depth, call.op_name.split(\"/\")[-1].split(\":\")[0], call.id)]\n",
    "\n",
    "    # Recurse into each child and extend the trajectory\n",
    "    for child in call.children():\n",
    "        trajectory.extend(traverse_children(child, depth + 1))\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "trace_id = \"01919262-5ab5-7521-94e9-6efa0ae9c794\"\n",
    "root_call = eval_call_trace_ids[trace_id]\n",
    "\n",
    "trajectory = traverse_children(root_call)\n",
    "trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories = []\n",
    "\n",
    "for trace_id, _ in eval_call_trace_ids.items():\n",
    "    root_call = eval_call_trace_ids[trace_id]\n",
    "    trajectory = traverse_children(root_call)\n",
    "    all_trajectories.append(trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def exact_match(model_output: list[tuple], trajectory: list[tuple]) -> float:\n",
    "    correct = sum(1 for p, g in zip(model_output, trajectory) if p[0] == g[0] and p[1] == g[1])\n",
    "    return correct / len(trajectory)\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def levenshtein_distance(model_output: list[tuple], trajectory: list[tuple]) -> float:\n",
    "    seq1 = [(p[0], p[1]) for p in model_output]\n",
    "    seq2 = [(g[0], g[1]) for g in trajectory]\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    return matcher.ratio()\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_predicted_trajectory(id: int) -> list[tuple]:\n",
    "    return all_trajectories[id]\n",
    "\n",
    "\n",
    "trajectory_evaluation = weave.Evaluation(\n",
    "    name=\"Trajectory Evaluation\",\n",
    "    dataset=GROUND_TRUTH_TRAJECTORY,\n",
    "    scorers=[exact_match, levenshtein_distance],\n",
    ")\n",
    "\n",
    "asyncio.run(\n",
    "    trajectory_evaluation.evaluate(get_predicted_trajectory)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate Factfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## factfulness scorer\n",
    "llm_factful_judge = \"\"\"You are an expert checker of facts. Given the context you can find if the generated text is based out of the context.\n",
    "If the generated text is coming from the provided context return \"CORRECT\", otherwise return \"INCORRECT\".\n",
    "\n",
    "Return your judgement in a valid JSON format:\n",
    "\n",
    "{\n",
    "  \"judgement\": \"CORRECT\" | \"INCORRECT\",\n",
    "  \"reasoning\": \"Reasoning for your judgement\",\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "llm_factful_judge_message = \"\"\"The provided context and generated text are:\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "GENERATED TEXT:\n",
    "{generated_text}\n",
    "\"\"\"\n",
    "\n",
    "@weave.op()\n",
    "def factfulness_evaluator(model_output: dict):\n",
    "    response = co.chat(\n",
    "        model=\"command-r-plus-08-2024\",\n",
    "        preamble=llm_factful_judge,\n",
    "        message=llm_factful_judge_message.format(\n",
    "            context=\" \\n\".join(model_output[\"context\"]), generated_text=model_output[\"generated_text\"]\n",
    "        ),\n",
    "        temperature=0,\n",
    "        prompt_truncation=\"OFF\"\n",
    "    )\n",
    "\n",
    "    # ideally run with retries or use structured output parsing\n",
    "    try:\n",
    "        eval = json.loads(response.text)\n",
    "        return {\n",
    "            \"score\": eval[\"judgement\"] == \"CORRECT\",\n",
    "            \"reasoning\": eval[\"reasoning\"]\n",
    "        }\n",
    "    except:\n",
    "        return {\n",
    "            \"score\": False,\n",
    "            \"reasoning\": response.text\n",
    "        }\n",
    "\n",
    "\n",
    "@weave.op()\n",
    "def get_context_and_answer(id: int) -> list[str]:\n",
    "    # we get the context from the 2nd last component of the trajectory\n",
    "    trajectory = all_trajectories[id]\n",
    "\n",
    "    if trajectory[-2][1] == \"python_interpreter\":\n",
    "        context = [weave_client.call(trajectory[-2][-1]).output[0][\"console_output\"]]\n",
    "\n",
    "    if trajectory[-2][1] == \"web_search\":\n",
    "        context = [web_result[\"content\"] for web_result in weave_client.call(trajectory[-2][-1]).output]\n",
    "\n",
    "    generated_text = weave.ref(weave_client.call(trajectory[-1][-1]).output).get().text\n",
    "\n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"generated_text\": generated_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factfulness_evaluation = weave.Evaluation(\n",
    "    name=\"Factfulness Evaluation\",\n",
    "    dataset=GROUND_TRUTH_TRAJECTORY,\n",
    "    scorers=[factfulness_evaluator],\n",
    ")\n",
    "\n",
    "asyncio.run(\n",
    "    factfulness_evaluation.evaluate(get_context_and_answer)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
